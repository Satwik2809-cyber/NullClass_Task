{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "391324c0b4fd4903ae7628b50bdb1756": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bb89243982474fcd92298ef435770248",
              "IPY_MODEL_0ee2c7051355454ca37463a06b93b602",
              "IPY_MODEL_f4775cd95cf64816bf59b36b8428eb0b"
            ],
            "layout": "IPY_MODEL_1406104bdb564e14b16c2764ba26708b"
          }
        },
        "bb89243982474fcd92298ef435770248": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e22eced382254b18954a956f34453b81",
            "placeholder": "​",
            "style": "IPY_MODEL_d264ba42ac41478d89541520c06f6b2f",
            "value": "Map:  41%"
          }
        },
        "0ee2c7051355454ca37463a06b93b602": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b404a3c8cd48454a90d853061c10a5f1",
            "max": 7357,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d37d38cdd47f41eca014b955cfe23322",
            "value": 2994
          }
        },
        "f4775cd95cf64816bf59b36b8428eb0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52d425c696104edeb5385cbaea9a2c20",
            "placeholder": "​",
            "style": "IPY_MODEL_b002a31c7cb64bf49045690ed793d04d",
            "value": " 2994/7357 [04:58&lt;00:54, 80.72 examples/s]"
          }
        },
        "1406104bdb564e14b16c2764ba26708b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e22eced382254b18954a956f34453b81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d264ba42ac41478d89541520c06f6b2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b404a3c8cd48454a90d853061c10a5f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d37d38cdd47f41eca014b955cfe23322": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "52d425c696104edeb5385cbaea9a2c20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b002a31c7cb64bf49045690ed793d04d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_O95wi6r23G",
        "outputId": "6f37cbab-6a0b-45b9-b977-444269707261"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m117.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.7/311.7 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m103.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m113.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m98.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gradio 5.42.0 requires huggingface-hub<1.0,>=0.33.5, but you have huggingface-hub 0.19.4 which is incompatible.\n",
            "sentence-transformers 5.1.0 requires huggingface-hub>=0.20.0, but you have huggingface-hub 0.19.4 which is incompatible.\n",
            "sentence-transformers 5.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.36.2 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2023.6.0 which is incompatible.\n",
            "peft 0.17.0 requires huggingface_hub>=0.25.0, but you have huggingface-hub 0.19.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q \\\n",
        "  diffusers==0.24.0 \\\n",
        "  transformers==4.36.2 \\\n",
        "  huggingface_hub==0.19.4 \\\n",
        "  accelerate==0.25.0 \\\n",
        "  datasets==2.14.5 \\\n",
        "  safetensors==0.4.0 \\\n",
        "  Pillow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, torch, torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from diffusers import StableDiffusionPipeline, DDPMScheduler\n",
        "from diffusers.models.attention_processor import LoRAAttnProcessor\n",
        "from huggingface_hub import hf_hub_download, model_info"
      ],
      "metadata": {
        "id": "J6DOIhZoszLk"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, io, glob, math\n",
        "from typing import Any\n",
        "from PIL import Image\n",
        "from torchvision.transforms import InterpolationMode\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "vnnnSGQr_gS1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_ID         = \"runwayml/stable-diffusion-v1-5\"\n",
        "LOCAL_PARQUET    = \"train-00000-of-00001.parquet\"    # downloaded file (optional)\n",
        "FALLBACK_DATASET = \"huggan/pokemon\"                  # public; no auth\n",
        "OUT_DIR          = \"sd15_pokemon_lora\"\n",
        "MAX_STEPS        = 1000\n",
        "BATCH_SIZE       = 2\n",
        "LR               = 1e-4\n",
        "IMG_SIZE         = 512\n",
        "GUIDANCE         = 7.5\n",
        "SEED             = 42\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DTYPE  = torch.float16 if DEVICE == \"cuda\" else torch.float32\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# -------------------\n",
        "# Transforms (SD expects [-1,1])\n",
        "# -------------------\n",
        "tf = transforms.Compose([\n",
        "    transforms.Resize(IMG_SIZE, interpolation=InterpolationMode.BICUBIC),\n",
        "    transforms.CenterCrop(IMG_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5],[0.5]),\n",
        "])\n",
        "\n",
        "# -------------------\n",
        "# Helpers: robust image normalization → PIL.Image\n",
        "# -------------------\n",
        "def to_pil(img_obj: Any) -> Image.Image:\n",
        "    \"\"\"\n",
        "    Convert various dataset image representations to a RGB PIL.Image:\n",
        "    - PIL.Image.Image\n",
        "    - bytes / bytearray\n",
        "    - dict with 'bytes' and/or 'path'\n",
        "    - str path\n",
        "    \"\"\"\n",
        "    if isinstance(img_obj, Image.Image):\n",
        "        return img_obj.convert(\"RGB\")\n",
        "    if isinstance(img_obj, (bytes, bytearray)):\n",
        "        return Image.open(io.BytesIO(img_obj)).convert(\"RGB\")\n",
        "    if isinstance(img_obj, dict):\n",
        "        b = img_obj.get(\"bytes\", None)\n",
        "        p = img_obj.get(\"path\", None)\n",
        "        if b is not None:\n",
        "            return Image.open(io.BytesIO(b)).convert(\"RGB\")\n",
        "        if p is not None:\n",
        "            return Image.open(p).convert(\"RGB\")\n",
        "    if isinstance(img_obj, str):\n",
        "        return Image.open(img_obj).convert(\"RGB\")\n",
        "    # Last-ditch: some datasets wrap a dict in another container\n",
        "    raise TypeError(f\"Unsupported image type for PIL conversion: {type(img_obj)}\")\n",
        "\n",
        "def map_record(ex):\n",
        "    \"\"\"\n",
        "    Standardize a dataset example to:\n",
        "      { 'pixel_values': tensor[-1..1], 'caption': str }\n",
        "    Picks a plausible image-like field automatically; synthesizes caption if missing.\n",
        "    \"\"\"\n",
        "    # try common image keys first\n",
        "    img_like = ex.get(\"image\", None) or ex.get(\"img\", None) or ex.get(\"image_bytes\", None) or ex.get(\"file\", None)\n",
        "    if img_like is None:\n",
        "        # pick first non-caption-like field as image\n",
        "        for k, v in ex.items():\n",
        "            if k.lower() not in {\"text\",\"prompt\",\"caption\",\"label\",\"labels\",\"class\"}:\n",
        "                img_like = v\n",
        "                break\n",
        "    if img_like is None:\n",
        "        raise KeyError(\"No image-like field found in example. Keys: \" + \", \".join(ex.keys()))\n",
        "\n",
        "    pil = to_pil(img_like)\n",
        "    cap = ex.get(\"text\") or ex.get(\"prompt\") or ex.get(\"caption\") or \"a cute pokemon creature, high detail\"\n",
        "    return {\"pixel_values\": tf(pil), \"caption\": cap}\n",
        "\n",
        "# -------------------\n",
        "# Dataset loader (local parquet → fallback to public)\n",
        "# -------------------\n",
        "def load_pokemon_dataset():\n",
        "    if os.path.exists(LOCAL_PARQUET):\n",
        "        ds = load_dataset(\"parquet\", data_files=LOCAL_PARQUET, split=\"train\")\n",
        "        ds = ds.map(map_record, remove_columns=ds.column_names)\n",
        "        return ds\n",
        "    ds = load_dataset(FALLBACK_DATASET, split=\"train\")\n",
        "    ds = ds.map(map_record, remove_columns=ds.column_names)\n",
        "    return ds\n",
        "\n",
        "dataset = load_pokemon_dataset()\n",
        "print(dataset)\n",
        "print(\"Example keys:\", dataset[0].keys())\n",
        "print(\"Example caption:\", dataset[0][\"caption\"][:80])\n",
        "\n",
        "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, drop_last=True)\n",
        "\n",
        "# -------------------\n",
        "# SD Pipeline + LoRA setup\n",
        "# -------------------\n",
        "pipe = StableDiffusionPipeline.from_pretrained(MODEL_ID, torch_dtype=DTYPE)\n",
        "pipe.scheduler = DDPMScheduler.from_config(pipe.scheduler.config)\n",
        "pipe = pipe.to(DEVICE)\n",
        "\n",
        "# freeze base; train only LoRA\n",
        "pipe.vae.requires_grad_(False)\n",
        "pipe.text_encoder.requires_grad_(False)\n",
        "pipe.unet.requires_grad_(False)\n",
        "\n",
        "# Attach LoRA adapters to all attention processors\n",
        "rank = 8\n",
        "attn_procs = {}\n",
        "for name, proc in pipe.unet.attn_processors.items():\n",
        "    cross_dim = getattr(proc, \"cross_attention_dim\", None)\n",
        "    hidden_size = getattr(proc, \"hidden_size\", pipe.unet.config.attention_head_dim)\n",
        "    attn_procs[name] = LoRAAttnProcessor(\n",
        "        hidden_size=hidden_size, cross_attention_dim=cross_dim, rank=rank\n",
        "    )\n",
        "pipe.unet.set_attn_processor(attn_procs)\n",
        "\n",
        "# optimizer on LoRA params only\n",
        "lora_params = [p for p in pipe.unet.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.AdamW(lora_params, lr=LR)\n",
        "\n",
        "# shortcuts\n",
        "tokenizer       = pipe.tokenizer\n",
        "text_encoder    = pipe.text_encoder\n",
        "vae             = pipe.vae\n",
        "unet            = pipe.unet\n",
        "noise_scheduler = pipe.scheduler\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\"))\n",
        "global_step, loss_acc = 0, 0.0\n",
        "pbar = tqdm(total=MAX_STEPS, desc=\"Training (LoRA)\")\n",
        "\n",
        "# -------------------\n",
        "# Training loop\n",
        "# -------------------\n",
        "while global_step < MAX_STEPS:\n",
        "    for batch in loader:\n",
        "        if global_step >= MAX_STEPS:\n",
        "            break\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # captions -> text embeddings\n",
        "            tok = tokenizer(\n",
        "                batch[\"caption\"], padding=\"max_length\", truncation=True,\n",
        "                max_length=tokenizer.model_max_length, return_tensors=\"pt\"\n",
        "            )\n",
        "            enc = text_encoder(tok.input_ids.to(DEVICE))[0]\n",
        "\n",
        "            # images -> latents\n",
        "            imgs = batch[\"pixel_values\"].to(DEVICE, dtype=DTYPE)\n",
        "            latents = vae.encode(imgs).latent_dist.sample() * 0.18215  # SD scaling\n",
        "\n",
        "        # add noise\n",
        "        noise = torch.randn_like(latents)\n",
        "        t = torch.randint(0, noise_scheduler.config.num_train_timesteps,\n",
        "                          (latents.shape[0],), device=DEVICE, dtype=torch.long)\n",
        "        noisy = noise_scheduler.add_noise(latents, noise, t)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\")):\n",
        "            pred = unet(noisy, t, encoder_hidden_states=enc).sample\n",
        "            loss = nn.functional.mse_loss(pred, noise)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        global_step += 1\n",
        "        loss_acc += loss.item()\n",
        "\n",
        "        if global_step % 50 == 0:\n",
        "            print(f\"step {global_step}: loss {loss_acc/50:.4f}\")\n",
        "            loss_acc = 0.0\n",
        "\n",
        "        if global_step % 250 == 0 or global_step == MAX_STEPS:\n",
        "            save_dir = os.path.join(OUT_DIR, f\"lora_step_{global_step}\")\n",
        "            os.makedirs(save_dir, exist_ok=True)\n",
        "            pipe.unet.save_attn_procs(save_dir)\n",
        "            print(\"Saved LoRA to:\", save_dir)\n",
        "\n",
        "        pbar.update(1)\n",
        "\n",
        "pbar.close()\n",
        "print(\"Training done!\")\n",
        "\n",
        "# -------------------\n",
        "# Inference with latest LoRA\n",
        "# -------------------\n",
        "chkpts = sorted(glob.glob(os.path.join(OUT_DIR, \"lora_step_*\")),\n",
        "                key=lambda p: int(p.split(\"_\")[-1]))\n",
        "assert len(chkpts) > 0, \"No LoRA checkpoints found!\"\n",
        "latest = chkpts[-1]\n",
        "pipe.unet.load_attn_procs(latest)\n",
        "print(\"Loaded LoRA:\", latest)\n",
        "\n",
        "prompt = \"a cute watercolor pokemon, pastel colors, high detail\"\n",
        "neg    = \"blurry, low quality, watermark\"\n",
        "images = pipe(prompt, num_inference_steps=30, guidance_scale=GUIDANCE,\n",
        "              negative_prompt=neg, num_images_per_prompt=2).images\n",
        "\n",
        "os.makedirs(\"samples\", exist_ok=True)\n",
        "for i, im in enumerate(images):\n",
        "    fp = f\"samples/poke_{i}.png\"\n",
        "    im.save(fp)\n",
        "    print(\"Saved:\", fp)\n",
        "\n",
        "print(\"All done ✅\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "391324c0b4fd4903ae7628b50bdb1756",
            "bb89243982474fcd92298ef435770248",
            "0ee2c7051355454ca37463a06b93b602",
            "f4775cd95cf64816bf59b36b8428eb0b",
            "1406104bdb564e14b16c2764ba26708b",
            "e22eced382254b18954a956f34453b81",
            "d264ba42ac41478d89541520c06f6b2f",
            "b404a3c8cd48454a90d853061c10a5f1",
            "d37d38cdd47f41eca014b955cfe23322",
            "52d425c696104edeb5385cbaea9a2c20",
            "b002a31c7cb64bf49045690ed793d04d"
          ]
        },
        "id": "Fqt5qYSitBi2",
        "outputId": "efb6d6cc-364a-4c25-d9b6-49515e2c609f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/7357 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "391324c0b4fd4903ae7628b50bdb1756"
            }
          },
          "metadata": {}
        }
      ]
    }
  ]
}